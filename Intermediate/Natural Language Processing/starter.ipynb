
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Intermediate Level\n",
    "\n",
    "Welcome to the Natural Language Processing intermediate tasks! This notebook contains three comprehensive tasks to test your understanding of text processing, embeddings, NER, and text generation.\n",
    "\n",
    "## Tasks Overview:\n",
    "1. **Task 1: Text Classification with Word Embeddings** - Build a classifier using embeddings\n",
    "2. **Task 2: Named Entity Recognition (NER)** - Identify entities in text\n",
    "3. **Task 3: Text Generation with RNNs** - Generate text using recurrent networks\n",
    "\n",
    "Please refer to `tasks.md` for detailed requirements for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import necessary libraries\n",
    "# Example:\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Text Classification with Word Embeddings\n",
    "\n",
    "Build a text classifier using pre-trained word embeddings (Word2Vec, GloVe, or FastText).\n",
    "\n",
    "**Requirements:**\n",
    "- Load a text classification dataset (e.g., IMDB reviews, news categories)\n",
    "- Preprocess text (tokenization, padding, etc.)\n",
    "- Use pre-trained embeddings or train your own\n",
    "- Build an LSTM or GRU-based classifier\n",
    "- Achieve at least 80% accuracy\n",
    "- Display confusion matrix and classification report\n",
    "\n",
    "**Hints:**\n",
    "- Download pre-trained embeddings from available sources\n",
    "- Use appropriate tokenization and sequence padding methods\n",
    "- Embedding layer should use pre-trained weights\n",
    "- Consider using bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 1 - Load and explore dataset\n",
    "# Load IMDB, news categories, or another text classification dataset\n",
    "# Explore the data distribution\n",
    "# Display sample texts and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 2 - Preprocess text\n",
    "# Tokenize text using Keras Tokenizer\n",
    "# Convert text to sequences\n",
    "# Pad sequences to uniform length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 3 - Load pre-trained embeddings\n",
    "# Download and load GloVe, Word2Vec, or FastText embeddings\n",
    "# Create embedding matrix for your vocabulary\n",
    "# Map words from tokenizer to embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4 - Build LSTM/GRU classifier\n",
    "# Create model with Embedding layer (using pre-trained weights)\n",
    "# Add LSTM or GRU layers (consider bidirectional)\n",
    "# Add Dense layers for classification\n",
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 5 - Train the model\n",
    "# Train your classifier\n",
    "# Monitor training and validation metrics\n",
    "# Aim for at least 80% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6 - Evaluate and visualize results\n",
    "# Generate predictions on test set\n",
    "# Create confusion matrix\n",
    "# Display classification report (precision, recall, F1-score)\n",
    "# Show sample predictions with actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Named Entity Recognition (NER)\n",
    "\n",
    "Implement a Named Entity Recognition system to identify entities in text.\n",
    "\n",
    "**Requirements:**\n",
    "- Use a labeled NER dataset (e.g., CoNLL-2003)\n",
    "- Build or use a pre-trained NER model\n",
    "- Identify at least 3 entity types (Person, Location, Organization)\n",
    "- Calculate precision, recall, and F1-score\n",
    "- Demonstrate on custom example sentences\n",
    "\n",
    "**Hints:**\n",
    "- Consider using pre-trained NER models from popular NLP libraries\n",
    "- For custom training, use BiLSTM-CRF architecture\n",
    "- Use BIO tagging scheme\n",
    "- Evaluate per-entity-type and overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 1 - Load NER dataset\n",
    "# Load CoNLL-2003 or similar NER dataset\n",
    "# Explore entity types and distribution\n",
    "# Display sample annotated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 2 - Preprocess data for NER\n",
    "# Tokenize and prepare sequences\n",
    "# Convert entity tags to numerical format (BIO scheme)\n",
    "# Create train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 3 - Build or load NER model\n",
    "# Option 1: Use spaCy pre-trained model\n",
    "# Option 2: Build BiLSTM-CRF model from scratch\n",
    "# Configure for sequence tagging task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4 - Train or fine-tune the model\n",
    "# If using custom model, train on NER dataset\n",
    "# If using pre-trained, you can skip or fine-tune\n",
    "# Monitor performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 5 - Evaluate the model\n",
    "# Calculate precision, recall, F1-score per entity type\n",
    "# Display overall metrics\n",
    "# Show confusion matrix if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6 - Test on custom sentences\n",
    "# Create custom example sentences\n",
    "# Run NER on them\n",
    "# Highlight identified entities with their types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Text Generation with RNNs\n",
    "\n",
    "Create a text generation model using Recurrent Neural Networks.\n",
    "\n",
    "**Requirements:**\n",
    "- Train on a text corpus (e.g., Shakespeare, song lyrics, or code)\n",
    "- Implement character-level or word-level generation\n",
    "- Use LSTM or GRU cells\n",
    "- Generate coherent text samples with temperature sampling\n",
    "- Compare outputs at different temperature values\n",
    "- Show the model's ability to learn patterns from the training data\n",
    "\n",
    "**Hints:**\n",
    "- Character-level is simpler, word-level produces better coherence\n",
    "- Use one-hot encoding or embeddings for input\n",
    "- Temperature controls randomness: low=conservative, high=creative\n",
    "- Train on enough data for meaningful patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 1 - Load and prepare text corpus\n",
    "# Load your chosen corpus (Shakespeare, lyrics, code, etc.)\n",
    "# Clean and preprocess the text\n",
    "# Decide on character-level or word-level approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 2 - Create training sequences\n",
    "# Generate input-output pairs for training\n",
    "# Use sliding window approach\n",
    "# Create vocabulary mapping (char/word to index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 3 - Build LSTM/GRU text generation model\n",
    "# Create model with Embedding (if word-level) or one-hot encoding\n",
    "# Add LSTM or GRU layers (stacked if needed)\n",
    "# Output layer with softmax for next token prediction\n",
    "# Compile with appropriate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4 - Train the model\n",
    "# Train on your prepared sequences\n",
    "# Monitor loss to ensure learning\n",
    "# Save model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 5 - Implement text generation with temperature\n",
    "# Create generation function with temperature parameter\n",
    "# Sample next token based on probability distribution\n",
    "# Generate sequences of desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6 - Generate and compare samples\n",
    "# Generate text with different temperatures (e.g., 0.2, 0.5, 1.0, 1.5)\n",
    "# Compare outputs: low temperature = conservative, high = creative\n",
    "# Display multiple samples from different seed texts\n",
    "# Analyze the model's learned patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
